{"cells":[{"cell_type":"markdown","source":["## lmsys/vicuna-13b-v1.3"],"metadata":{"id":"8-FyKmtF58yl"},"id":"8-FyKmtF58yl"},{"cell_type":"markdown","source":["In this project we experiment with [lmsys/vicuna-13b-v1.3]( https://huggingface.co/lmsys/vicuna-13b-v1.3) and observe the model performance and the way it generates prompt outputs. Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.\n","\n"," Vicuna-13B is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90% quality of OpenAI ChatGPT and Google BARD while outperforming other models like LLaMA and Stanford Alpaca in more than 90%.\n","\n","[ Learn more](https://lmsys.org/blog/2023-03-30-vicuna/)"],"metadata":{"id":"8xLabj5-2fx-"},"id":"8xLabj5-2fx-"},{"cell_type":"markdown","id":"9fbbf7dc","metadata":{"id":"9fbbf7dc"},"source":["### Import Libraries"]},{"cell_type":"markdown","id":"23f7193d","metadata":{"id":"23f7193d"},"source":["If transformer is not installed previously in your environment execute the following cell:"]},{"cell_type":"code","execution_count":null,"id":"610ee8ad","metadata":{"id":"610ee8ad"},"outputs":[],"source":["!pip install git+https://github.com/huggingface/transformers.git@refs/pull/25740/head accelerate"]},{"cell_type":"markdown","id":"8809cb70","metadata":{"id":"8809cb70"},"source":["If allready present, import the libraries"]},{"cell_type":"code","execution_count":null,"id":"3bb4a486","metadata":{"id":"3bb4a486"},"outputs":[],"source":["import transformers\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM"]},{"cell_type":"code","execution_count":null,"id":"6a6b3d1a","metadata":{"id":"6a6b3d1a"},"outputs":[],"source":["path =\"/home/kgupta/LLM/lmsys_vicuna_13b_v1_3\"\n","tok_path='/home/kgupta/LLM/lmsys_vicuna_13b_v1_3/tokenizer'"]},{"cell_type":"code","execution_count":null,"id":"cc0e5769","metadata":{"colab":{"referenced_widgets":["0f96a2afd0df40b7afafc288811dd577"]},"id":"cc0e5769","outputId":"f539d3e0-833d-43ee-dc68-af3cca3ee492"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f96a2afd0df40b7afafc288811dd577","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model =AutoModelForCausalLM.from_pretrained(path) # helps you load the NLP model easily in sequential manner"]},{"cell_type":"markdown","id":"ef2af5d1","metadata":{"id":"ef2af5d1"},"source":["AutoTokenizer is a class from the Hugging Face Transformers library that allows you to automatically load tokenizers for different pre-trained models without specifying the exact model type."]},{"cell_type":"code","execution_count":null,"id":"962154e8","metadata":{"id":"962154e8","outputId":"9d544f95-72f4-467a-c6f1-0438bda59e72"},"outputs":[{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]},{"data":{"text/plain":["('/home/kgupta/LLM/lmsys_vicuna_13b_v1_3/tokenizer/tokenizer_config.json',\n"," '/home/kgupta/LLM/lmsys_vicuna_13b_v1_3/tokenizer/special_tokens_map.json',\n"," '/home/kgupta/LLM/lmsys_vicuna_13b_v1_3/tokenizer/tokenizer.model',\n"," '/home/kgupta/LLM/lmsys_vicuna_13b_v1_3/tokenizer/added_tokens.json',\n"," '/home/kgupta/LLM/lmsys_vicuna_13b_v1_3/tokenizer/tokenizer.json')"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer = AutoTokenizer.from_pretrained(path)\n","tokenizer.save_pretrained(tok_path)"]},{"cell_type":"markdown","id":"ea84e6b0","metadata":{"id":"ea84e6b0"},"source":["### Create Pipeline"]},{"cell_type":"markdown","id":"bc4490ef","metadata":{"id":"bc4490ef"},"source":["- `transformers.pipeline()`:\n","\n","    This is a function provided by the Hugging Face Transformers library for easily creating pipelines for various NLP tasks. Pipelines streamline the process of applying pre-trained models to text without needing to write custom code.\n","\n","- `tokenizer=tokenizer`:\n","\n","    Here, you're passing the tokenizer object to the pipeline. This tells the pipeline how to tokenize the input text before passing it to the model.\n","    \n","- `model=model`:\n","\n","    This specifies the pre-trained model you want to use for text generation. The model variable should be an instance of a pre-trained language model (e.g., GPT-2, GPT-3, etc.).\n","    \n","- `torch_dtype=torch.float16`:\n","\n","    This sets the torch data type to float16. This means that the model will perform computations using half-precision floating-point numbers, which can lead to faster computation times and reduced memory usage.\n","\n","- `device_map=\"auto\"`:\n","\n","    This argument is specifying that the pipeline should automatically detect the appropriate device for computation.\n"]},{"cell_type":"code","execution_count":null,"id":"ae1e4494","metadata":{"colab":{"referenced_widgets":["d9b1eb147e904925ac2277aeef4bdf50"]},"id":"ae1e4494","outputId":"45a4b575-e05c-4839-98ed-f610cbe3b151"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9b1eb147e904925ac2277aeef4bdf50","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    tokenizer=tokenizer,\n","    model=model,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")"]},{"cell_type":"markdown","id":"a7a5fa62","metadata":{"id":"a7a5fa62"},"source":["Checking if GPU is working"]},{"cell_type":"code","execution_count":null,"id":"ebf477cc","metadata":{"id":"ebf477cc","outputId":"cf649cab-07d7-4b8c-8a0c-1330276cd3b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU is available\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","if device.type == 'cuda':\n","    print('GPU is available')\n","else:\n","    print('Using CPU')"]},{"cell_type":"markdown","id":"f9b9bb02","metadata":{"id":"f9b9bb02"},"source":["### Prompt - 1"]},{"cell_type":"code","execution_count":null,"id":"e8c3100a","metadata":{"id":"e8c3100a"},"outputs":[],"source":["system = f'''Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.\n","\n","'''\n","\n","prompt = f\"\"\"\\\n","### Human:Add a+ b\n","### Assistant:\\\n","\"\"\""]},{"cell_type":"markdown","id":"3b97ac90","metadata":{"id":"3b97ac90"},"source":["Create a sequence with your preferred parameters"]},{"cell_type":"markdown","id":"dac832fd","metadata":{"id":"dac832fd"},"source":["Parameters:\n","- `prompt`: The input prompt or starting text for text generation.\n","\n","- `do_sample=True`: Enables random sampling of the next token, introducing randomness in the generated text.\n","\n","- `top_k=10`: Controls the number of highest probability tokens considered in the sampling. Higher values make the generation more focused.\n","\n","- `temperature=0.5`: Controls the randomness of the generation. Higher values (e.g., > 1) make the output more random, while lower values (e.g., < 1) make it more deterministic.\n","\n","- `top_p=0.95`: Defines a threshold for the cumulative probability of the next token. Only tokens with a cumulative probability up to `top_p` will be considered.\n","\n","- `eos_token_id=tokenizer.eos_token_id`: Specifies the End-of-Sequence token ID. This helps the model know when to stop generating text.\n","\n","- `num_return_sequences=1`: Determines how many different sequences will be generated. In this case, it's set to 1.\n","\n","- `max_length=1024`: Sets the maximum length of the generated sequence.\n","\n","- `add_special_tokens=False`: Specifies whether to add special tokens like `[CLS]`, `[SEP]`, etc. to the generated output.\n","\n","- `repetition_penalty=1.1`: Controls how much the model avoids repeating the same token. Higher values (greater than 1) discourage repetition.\n"]},{"cell_type":"code","execution_count":null,"id":"b422e4cc","metadata":{"id":"b422e4cc"},"outputs":[],"source":["sequences = pipeline(\n","    prompt,\n","    do_sample=True,\n","    top_k=10,\n","    temperature=0.5,\n","    top_p=0.95,\n","    eos_token_id=tokenizer.eos_token_id,\n","    num_return_sequences=1,\n","    max_length=1024,\n","    add_special_tokens=False,\n","    repetition_penalty = 1.1\n",")"]},{"cell_type":"code","execution_count":null,"id":"119a7766","metadata":{"id":"119a7766","outputId":"b34031fd-db0c-4a5a-f456-5d4395f7bf75"},"outputs":[{"name":"stdout","output_type":"stream","text":["Result: ### Human:Add a+ b \n","### Assistant: Sure, here's the calculation for adding a and b using the + operator:\n","```makefile\n","a = 10\n","b = 5\n","result = a + b\n","print(result) # Output: 15\n","```\n","I hope that helps! Let me know if you have any other questions.\n"]}],"source":["for seq in sequences:\n","    print(f\"Result: {seq['generated_text']}\")"]},{"cell_type":"markdown","id":"0b8b79ce","metadata":{"id":"0b8b79ce"},"source":["### Prompt - 2"]},{"cell_type":"code","execution_count":null,"id":"91d443dc","metadata":{"id":"91d443dc"},"outputs":[],"source":["system = f'''Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.\n","\n","'''\n","\n","prompt = f\"\"\"\\\n","### Human: {system}\n","### Assistant:\\\n","\"\"\""]},{"cell_type":"code","execution_count":null,"id":"36eda287","metadata":{"id":"36eda287"},"outputs":[],"source":["sequences = pipeline(\n","    prompt,\n","    do_sample=True,\n","    top_k=10,\n","    temperature=0.5,\n","    top_p=0.95,\n","    eos_token_id=tokenizer.eos_token_id,\n","    num_return_sequences=1,\n","    max_length=4096,\n","    add_special_tokens=False,\n","    repetition_penalty = 1.1\n",")"]},{"cell_type":"code","execution_count":null,"id":"ae93fd05","metadata":{"id":"ae93fd05","outputId":"851a3011-4db6-449b-d3c2-ed740db7bdc1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Result: ### Human: Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.\n","\n","\n","### Assistant: Aloha! I recently had the opportunity to explore the beautiful islands of Hawaii and I'm excited to share my experience with you all.\n","\n","My journey began on the island of Oahu, where I was immediately struck by the rich culture and history of the area. One of the first things I did was visit the Pearl Harbor Memorial, which was both moving and educational. It's important to remember the events that led up to the United States' entry into World War II, and this memorial does an excellent job of honoring those who lost their lives.\n","\n","Next, I headed over to the Polynesian Cultural Center, which was one of the highlights of my trip. This interactive museum allowed me to learn about the different Polynesian cultures that make up Hawaii, as well as other Pacific Island nations. I got to participate in traditional dances, try my hand at navigating a Polynesian canoe, and even attend a luau, complete with roasted pig and live music. The center is dedicated to preserving these cultural traditions, and it was truly a unique and enriching experience.\n","\n","Of course, no trip to Hawaii would be complete without spending time on the beach. I spent a day lounging on Waikiki Beach, soaking up the sun and taking in the stunning views of the ocean. It's easy to see why this beach is so popular, and I highly recommend it for anyone looking for some rest and relaxation.\n","\n","Overall, my trip to Hawaii was filled with memorable experiences and breathtaking scenery. From learning about history and culture to simply enjoying the beauty of the islands, there's something for everyone in Hawaii. If you're planning a trip, I definitely recommend adding these must-see attractions to your itinerary.\n"]}],"source":["for seq in sequences:\n","    print(f\"Result: {seq['generated_text']}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}