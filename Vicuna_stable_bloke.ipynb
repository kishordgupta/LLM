{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "621c018a",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a004e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a8a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/kgupta/LLM/bloke_stable_vicuna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf759170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7a00cb9df448abb88b5f969f044ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModelForCausalLM.from_pretrained(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1ad7f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "801b9c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "### Human: Write a Python script for text classification using Transformers and PyTorch\n",
    "### Assistant:\\\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6aeab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: Write a Python script for text classification using Transformers and PyTorch\n",
      "### Assistant: import torch\n",
      "import transformers\n",
      "import numpy as np\n",
      "\n",
      "### Load the model and dataset\n",
      "model = transformers.BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
      "dataset = transformers.LineByLineTextDataset(\n",
      "    tokenizer=model.tokenizer,\n",
      "    header=None,\n",
      "    separator='-',\n",
      "    output_text=True\n",
      ")\n",
      "\n",
      "### Data processing\n",
      "data = []\n",
      "for row in dataset:\n",
      "    label = row[1]\n",
      "    input_ids = row[0].split('\\n')[0].replace(' ', '')\n",
      "    target = {input_ids: label}\n",
      "    data.append(target)\n",
      "\n",
      "### Train the model\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
      "num_epochs = 3\n",
      "for epoch in range(num_epochs):\n",
      "    for input_target, epoch in enumerate(data):\n",
      "        input_ids = input_target['input_ids']\n",
      "        target = input_target['target']\n",
      "        optimizer.zero_grad()\n",
      "       \n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "tokens = model.generate(\n",
    " **inputs,\n",
    " max_new_tokens=256,\n",
    " do_sample=True,\n",
    " temperature=1.0,\n",
    " top_p=1.0,\n",
    ")\n",
    "print(tokenizer.decode(tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4cf7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 18 08:18:48 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX 6000...  On   | 00000000:01:00.0 Off |                  Off |\n",
      "| 30%   32C    P8    26W / 300W |      8MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX 6000...  On   | 00000000:41:00.0 Off |                  Off |\n",
      "| 30%   35C    P8    28W / 300W |      8MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX 6000...  On   | 00000000:61:00.0  On |                  Off |\n",
      "| 30%   37C    P8    28W / 300W |    367MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    440970      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A    440970      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A    440970      G   /usr/lib/xorg/Xorg                128MiB |\n",
      "|    2   N/A  N/A    441212      G   /usr/bin/gnome-shell               42MiB |\n",
      "|    2   N/A  N/A    443278      G   ...2/usr/lib/firefox/firefox      156MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2437bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 444935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9720b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
